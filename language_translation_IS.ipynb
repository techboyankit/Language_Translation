import string 
import re
from numpy import array, argmax, random, take
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed
from keras.preprocessing import ModelCheckpoint
from keras.models import load_model
from keras import optimizers
import matplotlib.pyplot as plt
% matplotlib inline
pd.set_option('display.max_colwidth',200)

# First we will read the file using the function defined below.

def read_text(filename):
    # open the file
    file=open(filename, mode='rt', encoding='utf-8')
    
    # read all text
    text = file.read()
    file.close()
    return text
    
# Function to split the text into English-German pairs separated by '\n'and then split 
# these pairs into English and German

# split a text into sentences
def to_lines(text):
    sents=text.strip().split('\n')
    sents=[i.split('\t') for i in sents]
    return sents
    
data = read_text("/content/deu.txt")
deu_eng=to_lines(data)
deu_eng = array(deu_eng)

# first 50,000 sentences pairs will be the training set
deu_eng=deu_eng[:50000,:]

#Let's take a look at our data
deu_eng

#Text to Sequence Conversion:
# To feed our data in a Seq2Seq model,we need to 
# convert both the input and output sentences into 
# integer sequences of fixed length. Before that ,we need
# to calculate lengths of all sentences in two separate 
# English and German lists.
eng_1=[]
deu_1=[]

#feeding lists with sentence lengths
for i in deu_eng[:,0]:
    eng_1.append(len(i.split()))
for i in deu_eng[:,1]:
    deu_l.append(len(i.split()))
        
length_df = pd.DataFrame({'eng':eng_l,'deu':deu_1})
length_df.hist(bins = 30)
plt.show()

# We will vectorize our text data by using Keras's Tokenizer() class. It will turn
# our sentences into sequences of integers.Then we will pad those sequences with zeros 
# to make all the sequences of same length.

# function to build a tokenizer
def tokenization(lines):
    tokenizer=Tokenizer()
    tokenizer.fit_on_texts(lines)
    return tokenizer

# prepare english tokenizer
eng_tokenizer = tokenization(deu_eng[:, 0])
eng_vocab_size = len(eng_tokenizer.word_index) + 1

eng_length = 8
print('English vocabulary size: %d'% eng_vocab_size)

# process Deutch Tokenizer
deu_tokenizer=tokenization(deu_eng[:, 1])
deu_vocab_size=len(deu_tokenizer.word_index)+1

deu_length =8
print('Deutch Vocabulary size: %d' % deu_vocab_size)

# encode and pad sequences
def encode_sequences(tokenizer, length, lines):
    # integer encode sequences
    seq=tokenizer.texts_to_sequences(lines)
    # pad sequences with 0 values
    seq = pad_sequences(seq, maxlen=length, padding='post')
    return seq

# Model building 
# Splitting data into train and test set

from sklearn.model_selection import train_test_split
train,test = train_test_split(deu_eng,test_size=0.2, random_state=12)

# Sentence Encoding
# prepare training data
trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])
trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])

# prepare validation data
testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])
testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])

# define Seq2Seq model architecture
# build NMT model
def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):
    model = Sequential()
    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))
    model.add(LSTM(units))
    model.add(RepeatVector(out_timesteps))
    model.add(LSTM(units))
    model.add(RepeatVector(out_timesteps))
    model.add(LSTM(units, return_sequences=True))
    model.add(Dense(out_vocab, activation='softmax'))
    return model

# RMSprop optimizer (good choice for neural networks)
model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)
rms = optimizers.RMSprop(lr=0.001)
model.compile(optimizers, loss='sparse_categorical_crossentropy')
# sparse_categorical_crossentropy is used as a loss function to use the target sequence 
# without any hot encoding format

filename = 'model.h1.24_aashutosh'
checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, 
                            mode='')
history= model.fit(trainX,trainY.reshape(trainY.shape[0],trainY.shape[1], 1), epochs=5, batch_size=512,
                  validation_split = 0.2,
                  callbacks=[checkpoint], verbose=1)

# Let's compare the training loss and the validation loss
plt.plt(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['train','validation'])
plt.show()

# Prediction time
model = load_model('model.h1.24_aashutosh')
preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))

def get_word(n, tokenizer):
    for word,index in tokenizer.word_index.items():
        if index == n;
           return word
    return None

# convert predictions into text(English)
preds_text = []
for i in preds:
    temp = []
    for j in range(len(i)):
        t=get_word(i[j], eng_tokenizer)
        if j>0:
           if(t== get_word(i[j-1], eng_tokenizer)) or (t==None):
              temp.append('')
           else:
              temp.append(t)
        else:
           if(t==None): 
              temp.append('')
           else:
              temp.append(t)
     preds_text.append(' '.join(temp))

pred_df = pd.DataFrame({'actual':test[:,0], 'predicted' : preds_text})
pd.set_option('display.max_colwidth',200)
pred_df.head(15)
pred_df.tail(15)

